/// Demonstrate configurable VortexMetrics isolation modes

use anyhow::Result;
use arrow::array::{Int64Array, RecordBatch};
use arrow::datatypes::{DataType, Field, Schema};
use datafusion::prelude::SessionContext;
use std::fs;
use std::sync::Arc;
use vortexlake_core::{Field as VLField, Schema as VLSchema, VortexLake, VortexLakeWriteConfig};
use vortexlake_sql::table_provider::VortexLakeTableProvider;
use vortexlake_sql::profiling::execute_with_full_profile;
use vortexlake_sql::metrics_config::{MetricsIsolationMode, VortexMetricsConfig};

#[path = "common.rs"]
mod common;
use common::{init_test_logging, get_test_data_dir};

async fn create_demo_dataset() -> Result<String> {
    let base_dir = get_test_data_dir().join("metrics_demo");
    let _ = fs::remove_dir_all(&base_dir);
    fs::create_dir_all(&base_dir)?;

    let db_path = base_dir.join("demo_db");
    let db = VortexLake::new(&db_path).await?;

    let schema = Arc::new(Schema::new(vec![
        Field::new("id", DataType::Int64, false),
        Field::new("value", DataType::Utf8, false),
    ]));

    let vl_schema = VLSchema::new(vec![
        VLField::new("id", DataType::Int64, false),
        VLField::new("value", DataType::Utf8, false),
    ])?;
    db.create_table("demo", vl_schema).await?;

    let config = VortexLakeWriteConfig::new().with_row_block_size(50);
    let mut writer = db.writer_with_config("demo", config)?;

    // Create 200 rows: blocks of 50 rows each = 4 blocks
    let ids: Vec<i64> = (1..=200).collect();
    let values: Vec<String> = (1..=200).map(|i| format!("value_{}", i)).collect();

    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![
            Arc::new(Int64Array::from(ids)),
            Arc::new(arrow::array::StringArray::from(values)),
        ],
    )?;

    writer.write_batch(batch).await?;
    writer.commit().await?;

    Ok(db_path.to_string_lossy().to_string())
}

#[tokio::test]
async fn demonstrate_metrics_isolation_modes() -> Result<()> {
    init_test_logging("metrics_isolation_demo.log");
    println!("\n=== VortexMetrics Isolation Mode Demonstration ===\n");

    let db_path = create_demo_dataset().await?;

    // Test queries that access the same file multiple times
    let queries = vec![
        ("Query 1: id < 60", "SELECT id FROM demo WHERE id < 60"),     // ~60 rows
        ("Query 2: id < 120", "SELECT id FROM demo WHERE id < 120"),   // ~120 rows
        ("Query 3: id < 180", "SELECT id FROM demo WHERE id < 180"),   // ~180 rows
    ];

    println!("Dataset: 200 rows, 4 blocks (50 rows each)");
    println!("Same file accessed multiple times in different queries\n");

    // === DEMO 1: SessionAccumulated (Default) ===
    println!("ðŸ“Š MODE 1: SessionAccumulated (Default - Session-level metrics)");
    println!("   - Metrics accumulate across entire session");
    println!("   - Shows total file access patterns");
    println!("   - Useful for cache optimization and hotspot analysis\n");

    let session_config = VortexMetricsConfig::new()
        .with_isolation_mode(MetricsIsolationMode::SessionAccumulated);

    let mut session_ctx = SessionContext::new();
    session_ctx.register_table(
        "demo",
        Arc::new(VortexLakeTableProvider::new(&db_path, "demo").await?),
    )?;

    for (desc, sql) in &queries {
        let (results, profile) = execute_with_full_profile(&session_ctx, sql).await?;
        let result_count: usize = results.iter().map(|b| b.num_rows()).sum();

        println!("  {}: {} results", desc, result_count);
        // Note: profile.print() would show accumulated metrics
    }

    println!("\n  ðŸ” Key Observation:");
    println!("    - rows_decoded_session_total would show CUMULATIVE total");
    println!("    - Query 1: ~60 rows");
    println!("    - Query 2: ~60 + 120 = ~180 rows (accumulated)");
    println!("    - Query 3: ~60 + 120 + 180 = ~360 rows (accumulated)\n");

    // === DEMO 2: PerDataSource (Isolated) ===
    println!("ðŸŽ¯ MODE 2: PerDataSource (Isolated per-scan metrics)");
    println!("   - Each DataSourceExec gets its own metrics scope");
    println!("   - Shows accurate per-scan consumption");
    println!("   - Useful for detailed query analysis\n");

    let isolated_config = VortexMetricsConfig::new()
        .with_isolation_mode(MetricsIsolationMode::PerDataSource);

    for (desc, sql) in &queries {
        // Fresh context for each query = isolated metrics
        let mut isolated_ctx = SessionContext::new();
        isolated_ctx.register_table(
            "demo",
            Arc::new(VortexLakeTableProvider::new(&db_path, "demo").await?),
        )?;

        let (results, profile) = execute_with_full_profile(&isolated_ctx, sql).await?;
        let result_count: usize = results.iter().map(|b| b.num_rows()).sum();

        println!("  {}: {} results", desc, result_count);
        // Note: profile.print() would show per-query accurate metrics
    }

    println!("\n  ðŸ” Key Observation:");
    println!("    - rows_decoded_this_scan would show ACCURATE per-query values");
    println!("    - Query 1: ~60 rows (exact consumption)");
    println!("    - Query 2: ~120 rows (exact consumption)");
    println!("    - Query 3: ~180 rows (exact consumption)\n");

    // === SUMMARY ===
    println!("ðŸŽ¯ SUMMARY:");
    println!("");
    println!("SessionAccumulated (é»˜è®¤):");
    println!("  âœ… é€‚åˆï¼šæ–‡ä»¶çƒ­ç‚¹åˆ†æžã€ç¼“å­˜ä¼˜åŒ–ã€ç³»ç»Ÿç›‘æŽ§");
    println!("  âœ… æ˜¾ç¤ºï¼šæ•´ä¸ªsessionçš„æ–‡ä»¶è®¿é—®æ¨¡å¼");
    println!("  âŒ ä¸é€‚åˆï¼šç²¾ç¡®çš„å•æŸ¥è¯¢æ€§èƒ½åˆ†æž");
    println!("");
    println!("PerDataSource (éš”ç¦»):");
    println!("  âœ… é€‚åˆï¼šç²¾ç¡®çš„æŸ¥è¯¢æ€§èƒ½åˆ†æžã€è°ƒè¯•");
    println!("  âœ… æ˜¾ç¤ºï¼šæ¯ä¸ªæ‰«æçš„å‡†ç¡®æ¶ˆè€—");
    println!("  âŒ ä¸é€‚åˆï¼šè·¨æŸ¥è¯¢çš„æ–‡ä»¶è®¿é—®æ¨¡å¼åˆ†æž");
    println!("");
    println!("ðŸ’¡ å»ºè®®ï¼š");
    println!("  - ç”Ÿäº§ç›‘æŽ§ï¼šä½¿ç”¨ SessionAccumulated");
    println!("  - æŸ¥è¯¢è°ƒè¯•ï¼šä½¿ç”¨ PerDataSource");
    println!("  - å¯é…ç½®ï¼šæ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å¼");

    Ok(())
}
